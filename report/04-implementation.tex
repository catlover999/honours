\chapter{Implementation\label{chap:implementation}}
\section{Functional Requirements}

\begin{enumerate}
    \item The system must support the compilation and execution of a Rust-based WebAssembly module for data processing.
    \item The WebAssembly module shall accept data records and tags as input and return a modified version of the data record.
    \item The system must apply differential privacy techniques to data records using either Laplace or Gaussian noise distributions, as specified by configuration settings.
    \item Configuration settings for differential privacy parameters (such as mean, sensitivity, epsilon, and delta) must be customizable through TOML files associated with specific data tags.
    \item The system shall offer support for optional settings in the differential privacy configuration, including the choice of a random number generator seed and the unit of measurement for noise application (integer or float).
    \item Error handling within the WebAssembly module must be robust, allowing for graceful degradation in the presence of malformed data records or configuration files.
    \item The Docker environment for the project must be configured to support the Rust development environment, Fluent Bit with WASM support, and an evaluation stage with Jupyter Notebook integration.
    \item The Fluent Bit integration must include the capability to process and forward logs with the addition of differential privacy measures implemented in the WebAssembly module.
    \item The system should provide a method for ahead-of-time (AOT) compilation of the WebAssembly module to optimize performance based on the 'wasm\_optimization` configuration.
    \item The evaluation stage of the system must allow for the analysis and reporting of processed data within a Jupyter Notebook environment, with dependencies specified in a `requirements.txt` file.
\end{enumerate}

\section{Rust library implementation}

The Rust implementation of the DP mechanisms leverages the \texttt{rv} crate for generating noise from the Laplace and Gaussian distributions. The \texttt{serde} crate is used extensively for serializing and deserializing the TOML configuration files and the JSON-formatted data records. To deterministically seed the random number generator for testing purposes, the \texttt{DefaultHasher} from the \texttt{std::hash} module is used to convert a string seed into a 64-bit value that initializes the \texttt{StdRng} from the \texttt{rand} crate.

\subsection{Overview of data flow}
The data flow within the Rust library follows a straightforward path. First, the incoming data records and their associated tags are processed into Rust-native types. The tags are used to load the corresponding configuration settings from TOML files. If a valid configuration is found for a given record, the library applies the specified DP mechanism (Laplace or Gaussian) to the numeric fields based on the loaded settings.

The noise generation process involves creating an instance of the chosen distribution using the parameters from the configuration (e.g., sensitivity, epsilon, delta). A random sample is then drawn from this distribution using either a default or user-specified random number generator. The resulting noise is added to the original value, and the perturbed record is returned as a JSON-formatted string.

If any errors occur during the processing, such as invalid configurations or non-numeric data, the library logs the issue using the \texttt{log} crate and returns the original record unmodified. This ensures graceful degradation and prevents data loss in case of misconfiguration or unexpected input.

\subsection{Loadable settings}
The DP library supports flexible configuration through TOML files. Each file corresponds to a specific tag and contains settings for one or more data streams associated with that tag. The settings include the choice of DP mechanism (Laplace or Gaussian), the privacy parameters (e.g., sensitivity, epsilon, delta), and optional configuration for the random number generator and output format.

The TOML files are deserialized into Rust structs using the \texttt{serde} crate. The structs use serde attributes to specify field names, default values, and flattening behavior. For example, the \texttt{Noise} enum represents the choice of DP mechanism and contains fields specific to each variant (e.g., \texttt{Laplace} and \texttt{Gaussian}). The \texttt{OptionalSettings} struct holds settings that are common to all mechanisms, such as the RNG seed and output format.

During the deserialization process, the library checks for missing or invalid settings and applies default values where necessary. This allows for concise configuration files that only specify the required parameters for each data stream.

\subsection{Adding noise to a record}
The core functionality of the DP library is adding noise to numeric values within a data record. This is implemented in the \texttt{add\_noise\_to\_value} function, which takes a generic \texttt{Distribution} enum and a \texttt{f64} value as input, along with an \texttt{OptionalSettings} struct for RNG seeding and output formatting.

Inside this function, a random number generator is instantiated based on the provided seed (if any). The seed is hashed using the \texttt{DefaultHasher} to obtain a 64-bit value, which is then used to initialize a \texttt{StdRng} from the \texttt{rand} crate. If no seed is specified, the default thread-local RNG is used instead.

Next, a random sample is drawn from the specified distribution using the \texttt{Distribution::draw} method from the \texttt{rv} crate. This returns a generic \texttt{Rv<f64>} value, which is converted to a plain \texttt{f64} using the \texttt{into} method.

Finally, the noise is added to the original value, and the result is converted to either an integer or floating-point representation based on the \texttt{unit} setting in \texttt{OptionalSettings}. The perturbed value is returned as a \texttt{serde\_json::Number}, which can be easily integrated into a JSON record.

The \texttt{add\_noise\_to\_record} function applies this process to each numeric field in a record, using the configuration settings loaded from the corresponding TOML file. The function skips over any fields that are missing from the configuration or have invalid settings, logging a warning message for each skipped field.

Overall, the Rust library provides a modular and extensible implementation of differential privacy mechanisms, with support for flexible configuration and error handling. The use of external crates like \texttt{rv}, \texttt{serde}, and \texttt{rand} simplifies the code and provides well-tested and optimized functionality for key operations like noise generation and serialization.

\subsection{OpenDP intergration issues}
A number of issues were incounted when trying to integrate OpenDP into the WASM environment. First the documentation is not fully up-to-date as for the past few dozen builds the automatically generated API documentation hosted on docs.rs failed to generate due to document compilation errors. OpenDP cannot run with it's default feature set in WASM it it has an optional dependency on openssl-sys which doesn't support WASM platforms. The good thing is, OpenSSL is only used once in OpenDP's codebase and is wrapped in a feature flag which can be disabled if you disable the default feature set when specifying the OpenDP dependency in your Cargo.toml file. The next issue that was faced was that in the latest version of OpenDP, the 
\section{WASM Filter plugin}
Fluent Bit has support for Filter and Input plugins in the form of loading a WASM binary and executing a configurable function inside the binary using a fixed ABI. That ABI corresponds to a single event containing a tag (and tag length), time (both seconds and nanoseconds), and a message (and message length). The message in the current stable release (as of v2.2.2) is a JSON-formatted string containing a list of key, value pairs corresponding 

\section{WASM limitations}
\subsection{Variable scope}
The choice of going with a WASM filter plugin does pose additional challenges compared to writing a native plugin. One of the main issues is pertaining to variable scope. On each evocation of a WASM function, a new instance is initialized by WAMR. Thus each instance is functionally stateless. Each settings file, where one settings file exists per Fluent Bit event, isn't internally cached by either the filter plugin itself, WAMR, or Fluent Bit. Therefore the only potential file caching that may occur exists in kernel-level filesystem read cache (details depend on the underlying operating system and backing-filesystem in-use). Regardless if any read cache could exist, it could currently only be implemented beneath the scope that could be controllable by a filter plugin.

\subsection{RNG Seeds}
The filter plugin implements an optionally seedable Pseudorandom number generator, however in practice, the application will only ever make one draw of the RNG seed per event per record. As all settings are shared between all records of the same key in all events with the same tag.

It could theoretically be possible to retain RNG-state in order to achieve both determinism and pseudorandomness between records, however an alternative mechanism would have to be developed to pass around this information. With the current limitations, it wouldn't be possible to have a shared variable, but it could be possible to store state either on the filesystem or as an additional record in the event. The first option would be the easiest to implement and likely most practical. You could serialize the rng state into a tempory file, which in term would be read, drawn, and written back on each subsequent filter invocation. However that could lead to undesirable performance characteristics as this is likely to greatly increase the IO demands of the Fluent Bit pipeline; with potential locking contention as the kernel attempts to order reads and writes.

The alternative is to route the RNG state through the Fluent Bit logging infrastructure. This would add significant complexity to the Fluent Bit datapipeline and may not be achievable in a desirable form. If a new record were to be created (or a set RNG record altered) for that to influence the following record a later filter stage would likely be needed to extract that particular RNG record. As Fluent Bit is first and foremost concerned with stateless data pipelines, requiring that a following event to only be processed after the current event has gone through several filter stages is a challenging task to solve. 

However, in real production usecases, seedable RNGs are unlikely to be needed. And if a seedable RNG is desirable for a particular usecase (such as having a deterministic output to validate some condition), then it is unlikely to matter if the pseudorandomness is static between records of the same setting and seed. A possible usecase which wouldn't be compromised by this limitation is to validate if a pseudorandomly generated noise value is consistent with a known good value generated for integration into a system's Unit testing infrastructure. 


\section{Header and data formatting for ingesting evaluation data}
\subsection{Line Splitting} \label{splitting}
In the sample evaluation Dockerfile, the input files are split into 200 line chunks before being read by Fluent Bit (however for the later Jupyter Notebook-based analysis, the original unsplit files are used). This is because if you read the whole files directly, you encounter some memory allocation issue within Fluent Bit itself. This is unlikely to be caused by the filter plugin directly as one of the ways this issue manifests is with an empty string being sent to the \texttt{records} argument of C-ABI compatible function. This causes a panic as the filter is written with the assumption that the caller should provide it with a valid JSON string (which should account with arbitrary input into Fluent Bit, as Fluent Bit is the one responsible for creating valid JSON strings). I believe this issue must be occurring either in the Tail input plugin (which the evaluation code uses to feed data from the sample CSV) or in Fluent Bit's WAMR based WASM integration. 

This should have no barring on working with larger inputs in real-world situations, as typically input sources are not static files but dynamic log sources. The solution found to get around from this limitation involves having the Tail Input plugin to read multiple files at once. Another clue that is potentially a Tail problem is that even though reading multiple files, it is doing so in the same worker instances, and directing the events (seemingly in-order from all my testing) to a single-worker WASM filter plugin instance (though note, with how I setup the dataflow of ) 

In initial testing a Python script was integrated into the Dockerfile build progress, however upon diagnosing the exact cause of the issue, it was decided to switch with using the \texttt{split} utility provided by the GNU \texttt{coreutils} package as it achieved all stated aims while being easily adaptable to introduce new text files if ever so desired into the current evaluation architecture.

\lstinputlisting[firstline=66, lastline=67, caption={Dataset file splitting}]{Dockerfile}
\subsection{CSV file ingest}
While creating the evaluation part of the project, much time was sent as to how individual lines of the sample datasets should be interpreted by the application. As of the time of writing, Fluent Bit doesn't provide native support for interpreting \acrshort{csv} files however a Merge Request was previously made in \href{#5040}{https://github.com/fluent/fluent-bit/pull/5040} to add such an Input plugin however it was closed its author April 8th 2022 and there appears to be no movement since. The same author however added initial \acrshort{csv} support in Fluent bit with Merge Request \href{#5269}{https://github.com/fluent/fluent-bit/pull/5269} and subsequently published from Fluent Bit \href{v1.9.3}{https://fluentbit.io/announcements/v1.9.3/}. It doesn't appear that this adds \acrshort{csv} support for as a Parsers Format as described in \#5040. It is unclear how the functionality of this patch is actually used. Unless an alternative solution comes to fruition, the simplest way to ingest a static CSV file into Fluent Bit for offline processing is through configuring a parser manually.

Static processing of each line of the CSV file may have more or less overhead than would be typical for such case. This is because what is referred to by "static processing" is through parsing each line of the csv file through a Regex parser in order to match values to their associated keys. The Regex used matches all content till each subsequent comma as to belonging to the prescribed key, till the last value which takes the remainder of the line. This is specified in the \texttt{parsers.conf} file which needs to be specified in the \texttt{[SERVICE]} section of the Fluent Bit configuration file. Fluent Bit uses the Onigmo library internally to handle Regex expressions. Onigmo compiles the given Regular Expression into an efficient bytecode-based state machine\cite{Hess_2023}, therefore, it shouldn't be any less efficient than a non-Regex approach. Fluent Bit evaluates all Regex expressions once at start time, therefore, even though streaming data through Regex is often given a bad wrap, it shouldn't pose any bottlenecks to runtime performance.

There is however CSV support currently in the Output File plugin, however this lead to significant issues for my situation as Fluent Bit doesn't deterministically store records in the same order. As mentioned in \ref{splitting}, it became pertinent to ingest the evaluation example events through reading multiple smaller chunk files. However it was desirable to have all those chunk files to be written into a single output file for the original unsplit input files. However, it was found that sometimes Fluent Bit would decide on a different order for each of the columns based on which source chunk file it was deriving from. A solution for this was to write CSV headers to each of the input chunks so that the columns could be reordered by Pandas in the Jupyter Notebook. No option is documented to set set the headers or column order of the included CSV format. While Pandas does have support for multiple header rows, it was quickly decided upon that this wasn't an ideal approach. Additionally the option would encapsulate every string in double quotations, while inconsequential as Pandas has an option for removing quotation marks, it wasn't desirable either. Using Fluent Bit's File Output plugin with Format set to CSV was found to have too many problems for what it is worth for my specific scenario. I am therefore using the "template" format instead which allows me to define an arbitrary format. In this case, in the form of a statically defined CSV-complient format as below:

\lstinputlisting[language=yaml, firstline=25, lastline=38, caption={Example dataset: File plugin configurations}, label={code:out-file}]{fluent-bit.yaml}

Despite the issues that occurred when writing events from multiple input sources into a single output source, the "format csv" option for the \texttt{file} plugin is still useful as it can support encoding files as with arbitrary keys. This can allow for more flexible ingesting of data as the data schema isn't hardcoded into the Fluent Bit configuration file.

\subsection{Data corruption issues - interim solution}
Somewhere in the data pipeline records were being corrupted, that being, an "empty" record was somehow being passed though the pipeline that due to the lack of corresponding keys, lead to several events being written to the output file as "\texttt{\seqsplit{{\{time\}},{\{Department\}},\{{Department\_Name\}},{\{Division\}},{\{Gender\}},{\{Base\_Salary\}},{\{Overtime\_Pay\}},{\{Longevity\_Pay\}},{\{Grade\}}}}".

This was detrimental as even without any settings created to perturb the desired data, the mean, count, and sums already different between input and output datasets making it harder to properly evaluate the effects of the additive noise being applied. Additionally, having strings in non-strings columns led Pandas to interpret each column as being of an "object" dtype instead of being "int64" or "float64" Panda dtypes. The interim solution was to assign each row in the input data a unique identifier and then discard any row of the input dataset in the evaluation notebook. This ensured that only the data that was actually perturbed would be used for analysis though had the consequence of dropping around 33 events. Privacy implications of having a unique identifier per row aren't significant as that column wouldn't be an exported nor utilized field after weeding out the corrupt entries. The act of adding an ID to each row was first achieved through a Python script being run to preprocess the input dataset before running the Dockerfile. Following a refactor this Python script was combined with the one for splitting lines as detailed in \ref{splitting} before this was replaced with an \texttt{awk} command to achieve the same ends. The \texttt{EmployeeSalaries.csv} was the only one affected, possibly due to it's longer total length compared to \texttt{StudentsPerformance.csv} where the issue failed to manifest.

However, after code refactoring the issue of corrupted lines went away. It is my hunch that the underlying cause can be attributed to the parsing of header files and possibly of the last line of each of the split input file chunks being interpreted when they shouldn't have been. Possibly due to the different behaviours of how GNU \texttt{split} and Pandas write \acrshort{csv} files. Therefore all such ID adding and corrupt line filtering was removed in the final itteration of the project. 

\section{OCI implimentation details}
\subsection{Rust build stage}
The final Dockerfile includes a fairly standard build stage to build the Rust filter plugin. It is based off of the latest upstream Rust image and pulls in the wasm32-wasi target through the rustup package manager. It optionally supports specifying the target cargo should build through a Docker buld argument. As the filter plugin is a library, it deliberately doesn't track the \texttt{Cargo.lock} file and uses deliberately broad package dependency definitions with the aim of always building the plugin with the latest upstream packages. This is to catch any upstream breaking changes quickly in the build cycle wherever possible. It was chosen early on to not use the lighter-weight \texttt{rust-alpine} image due to concerns over Fluent Bit's lack of support for running with \texttt{musl libc}. However as \text{GNU libc} isn't even in use due to the WASM platform (neither is \texttt{wasi-libc} as I'm not using the \texttt{libc} Rust crate in the filter plugin).

In its current form, there is no dependency nor build artifact caching beyond what Docker provides. However early on in development \texttt{cargo-chef} was trialled as a potential tool to reduce build times. It works on the principle of installing all dependencies in one Docker layer before building in another layer. However, even after specifying the desired target both with normally with \texttt{cargo build} but also with \texttt{cargo-chef} (so that it would prepare the correct dependencies), this added unforeseen issues in the final compiled binary. For whatever reason the final compiled WASM was significantly smaller than it should be and emitted difficult-to-diagnose errors when called on by Fluent Bit. It's hard to say what went wrong as the project's provided example configuration was followed and \texttt{cargo build} produced a binary without issue. Removing the \texttt{cargo-chef} infrastructure proved to simplify the Rust building code along with removed the associated breakage. It might not be a \texttt{cargo-chef} error per say as similar behaviour was seen when using the technique of running \texttt{cargo build} with only the associated \texttt{Cargo.toml} file along with a "bank" \texttt{lib.rs} file (only containing an empty main function)
It would be however possible to utilize the functionality provided by \texttt{docker buildkit cache} or alternative/derivative caching infrastructure (such as Caching in GitLab CI/CD)

\subsection{Fluent Bit build stage}
The primary motivation for building Fluent Bit ourselves instead of relying wholly on the official (debug) image is to build with the \texttt{DFLB\_WAMRC=On} cmake flag. This flag compels the compiler to create the \texttt{flb-wamrc} binary, which is the WAMR wamrc ahead-of-time compiler with Fluent Bit-specific modifications. This binary is not included in the upstream Fluent Bit image.

The secondary motivation is that despite Fluent Bit making official Aarch64 Docker images available, they use the default settings for \Gls{jemalloc}. Jemalloc has decided that due to some compiler and struct-size optimizations that can be had when assuming a page size, they would rather have those potentially marginal performance gains over having an ARM-spec conformant library that supports all systems. For context, Aarch64 supports 4k, 16k and 64k page sizes. 4k is currently the most widely used on ARM along with other architectures (most notably, all X86 systems). Some server-grade ARM chips are built using a 64k page size supporting processor. If an application is compiled to have a 64k-aligned page size, which is the default across all compilers, it will work with no modification on any system with using a smaller page size. However, you can set a smaller maximum page size (16k which will work on 16k and 4k or 4k which will only work on 4k). Jemalloc supports compiling to a 64k page size, however the default behaviour is to take the current system page size (which will typically be 4k). Upstream has been notified that the default is unwise as it renders binaries unportable with any system featuring a larger page size. However, they have so far refused to change the default setting to allow portability. Some downstream packages have overridden this default, some have not. Fluent Bit's Official Docker Images have not changed the default Jemalloc behaviour, therefore you must build Fluent Bit's Docker image locally on Apple Silicon Macs (this project was primarily developed on an X86 machine, however, some development occurred on a separate Apple Silicon Mac running Asahi Linux where this issue arose). 

The Fluent Bit build stage was loosely based on the official Fluent Bit Dockerfile. Therefore it is possible to substitute the \texttt{fluent} build stage with the upstream debug image. More details on this can be found in Appendix \ref{chap:user_man}. This does mean that you need to forego the \acrlong{aot} feature by setting the \texttt{wasm\_optimization} to \texttt{wasm} as a build argument. This may be desirable if you do not wish to wait for Fluent Bit to build on your machine. Do note only the debug images are supported as we need a working shell and not a distroless environment. 

\subsection{Fluent Bit execution stage}
In this stage, Fluent Bit is exececuted after some input data preprocessing. More details about the data preprocessing can be found in \ref{splitting}. Special arrangements were needed to get Fluent Bit to execute within the Docker build stage which would not be found in production.

Fluent Bit is designed to be always running. Either in the foreground when executing the \texttt{fluent-bit} binary directly or as a \Gls{daemon} in the background. This is because in all designed deployments of Fluent Bit, it is supposed to be continuously monitoring an input source, before filtering, buffering, and sending on \glspl{event} to their final destination. What Fluent Bit is not designed to do is to run with a defined finite input and then stop once that input has been fully read and processed. One potential solution is to run Fluent Bit in the background (using \texttt{bash}'s built-in \texttt{\&} operator) before \texttt{wait}ing a predetermined number of seconds for all processing to occur before \texttt{kill}ing the background running process. That has the drawback that it doesn't automatically scale with the increase in data that you feed it and that the processing time can vary between machines, leading to potential undefined behaviour on other systems. 

The finally solution chosen to combat this problem involves having a "control event". This event is placed at the end of one of the input files (in our case, \texttt{StudentsPerformance.csv}). What's important is that this control event contains the string "Quit". The current control string is as follows:

\begin{lstlisting}[caption={Quit Control code line}]
Quit,Quit,Quit,Quit,Quit,0,0,0
\end{lstlisting}

Only one instance of "Quit" is actually required; however, more doesn't hurt. In the main Fluent Bit config file, not only are the perturbed results written to an output file as seen in Listing \ref{code:out-file} but all events are also sent to \texttt{stdout}.
\lstinputlisting[language=yaml, firstline=40, lastline=41, caption={Usting the stdout plugin which matches all events}]{fluent-bit.yaml}
This could probably be reworked to reduce the amount of data sent to stdout by using an additional filter plugin stage (such as the "Rewrite Tag" or "grep" plugins) to send only the control sequence to stdout. However this is only a problem while evaluating the plugin on static data and not in-production.

All stdout of Fluent Bit is piped to GNU \texttt{grep}, which will match the first found instance of the control sequence. Once found, it will \texttt{sleep} for 5 seconds (for additional safety that the pipeline has been fully flushed, as Fluent Bit doesn't place a guarantee that non-connected pipeline stages will execute in a determined order). This wait may be shortened or lengthened based on evaluation requirements, though it's better to wait in order not to lose any events that haven't been sent to the Tail plugin yet. Once the wait is over, a SIGTERM signal is sent to Fluent Bit process. This is gracefully handled by Fluent Bit, which will start its shutdown sequence that is set to take a maximum of 5 seconds. An internal watchdog mechanism automatically intervenes if the shutdown process is not completed within this designated period. This watchdog is likely configured to send a more forceful termination signal, such as SIGKILL, to ensure that the application terminates promptly, preventing any potential hang or deadlock situations. This is essential as for a \texttt{RUN} command to complete successfully, it's constituent shell executed script must return without error.
\lstinputlisting[firstline=71, lastline=76, caption={Fluent Bit Execution Environment}]{Dockerfile}

\subsection{Jupiter Notebook}
The Dockerfile then finally pulls in the Jupyter Notebook Docker Stack from the \texttt{jupyter/scipy-notebook} image. This provides a full Jupyter Notebook server that can be accessed through the localhost link printed after the built container is started with podman run or docker run. Do note it's insufficient to go to localhost:8888 directly as the default Jupyter Notebook Docker Stack setting is token-authenticated. This token is embedded in the printed URI. 

With your chosen browser, the link will present you with a VS Code-like environment. Once navigating to "\texttt{project.ipynb}", one can then go to "Run>Run All Cells" in the header bar. This will rerun all the evaluation code.

The evaluation code currently uses Pandas, Numpy, and Pyplot from Matplotlib, however this is a prime area of future expansion to incorporate more data processing libraries such as OpenDP to perform further mesament to quantify the privacy gain and utility loss. Pandas, Numpy, and Matplotlib are all already included in the \texttt{jupyter/scipy-notebook} image. A previous itteration of the Dockerfile copied in a \texttt{requirements.txt} before running pip with the file as an argument. However as no dependencies were needed beyond what's already included in a standard environment, therefore such an approach of coping around requirements files should only be implemented in a scenario of expanded scope.


\subsection{Overview of OCI build stage flow}
\begin{tikzpicture}[
    node distance=1.5cm and 2cm,
    every node/.style={draw, rectangle, align=center, minimum height=2em, font=\sffamily},
    edge label/.style={midway, fill=white, font=\sffamily\small}
]
  % Nodes
  \node (builder) {Rust Build\\(rust:latest)};
  \node[below=of builder] (fluent) {Fluent Bit Setup\\(debian:bookworm-slim)};
  \node[right=of fluent] (fluent_wasm) {WASM Plugin\\(fluent-wasm)};
  \node[below=of fluent_wasm] (fluent_aot) {AOT Compilation\\(fluent-aot)};
  \node[left=of fluent_aot] (fluent_runner) {Fluent Runner\\(fluent-\{wasm\_optimization\})};
  \node[below=of fluent_runner] (notebook) {Jupyter Notebook\\(jupyter/minimal-notebook:latest)};

  % Edges
  \draw[->] (builder) -- node[edge label] {Copy WASM} (fluent_wasm);
  \draw[->] (fluent) -- (fluent_wasm);
  \draw[->] (fluent_wasm) -- node[edge label] {Convert to AOT} (fluent_aot);
  \draw[->] (fluent_wasm) -- (fluent_runner);
  \draw[->] (fluent_aot) -- (fluent_runner);
  \draw[->] (fluent_runner) -- node[edge label] {Copy Output} (notebook);
\end{tikzpicture}



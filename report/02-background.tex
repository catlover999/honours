\chapter{Background \& Related Work\label{chap:background}}

\section{Differential Privacy}

\acrfull{dp} is concerned with providing tools to quantitatively measure privacy in a dataset. Noise is added to individual records in a controlled way to not diminish utility beyond desired bounds.\cite{WhatisDifferentialPrivacy} Striking the correct balance between privacy and utility is at the heart of \acrshort{dp} research.\cite{Dwork2014} \acrshort{dp} can help aid regulatory compliance with privacy regulation. It has proven itself a useful tool and is widely deployed usually using bespoke implementations. It is used to anonymize telemetry data on iOS, Android, Windows, and Chrome.\cite{Maranon,markdefalco_2020} However, most existing implementations don't have a high degree of portability, which this project aims to improve. Active research is conducted through both both from academia and industry. Notable groups engaged in the field are Microsoft's AI Lab, Google's Research, Harvard's Privacy Tools Project, Apple's Machine Learning Research - Differential Privacy Team. 

\acrlong{dp} has emerged as the de facto standard for privacy-preserving data analysis, providing a framework to quantify and control the privacy loss incurred when statistical analyses are performed on sensitive data. The seminal work by \citep{Dwork2006calibrating} introduced the concept of \acrshort{dp} and proposed a mechanism for privacy-preserving statistical database queries by adding calibrated noise to the query outputs. This work demonstrated that privacy could be preserved by adjusting the noise scale according to the sensitivity of the function being computed, ensuring that the presence or absence of any single individual in the dataset does not significantly affect the outcome of the analysis. This foundational principle is at the heart of differential privacy and guides the development of privacy-preserving technologies.

The concept of \acrlong{dp} was formalized by Dwork et al., establishing a framework to protect individuals' privacy when their data is included in statistical analyses. The introduction of \acrshort{dp} provided a robust mathematical definition of privacy guarantees, applicable irrespective of external information sources\cite{Dwork2006calibrating}. This foundational work laid the groundwork for subsequent research in the area, defining $\epsilon$-differential privacy, a parameter that quantifies the privacy loss in data analysis outputs.

Subsequent research by Dwork et al. further explored the mechanics of \acrshort{dp}, focusing on the design of mechanisms that add noise to the outputs of database queries based on their sensitivityâ€”a measure of how much a query's outcome can be altered by changing a single individual's data in the database\cite{Dwork2006calibrating}. This work introduced the Laplace mechanism, a method for adding noise scaled to the query's sensitivity, ensuring that the presence or absence of any individual's data does not significantly affect the query's output.

The Gaussian mechanism, another pivotal contribution to \acrshort{dp}, was introduced to address scenarios where the Laplace mechanism's noise distribution was not ideal due to its heavy tails. The Gaussian mechanism provides an alternative that, under certain conditions, offers better trade-offs between privacy and accuracy, particularly in the context of numeric data analysis\cite{Dwork2014}.

In addition to these core mechanisms, the concept of the privacy budget was introduced, highlighting the cumulative nature of privacy loss across multiple queries and the importance of managing this budget to maintain robust privacy guarantees over time\cite{McSherry2007}.

Theoretical advancements have also been made in understanding the composability of differential privacy, demonstrating how multiple \acrshort{dp} mechanisms can be combined while quantitatively analyzing the resulting overall privacy guarantee. This work has been crucial for developing complex, privacy-preserving analytical frameworks\cite{Dwork2014}.

\subsection{Global vs Local Differential Privacy}
Lastly, the concept of global and local differential privacy has differentiated between privacy guarantees provided by a trusted curator (global \acrshort{dp}) versus those provided in a decentralized manner, where individuals perturb their data (local \acrshort{dp}). This distinction has broadened the applicability of \acrshort{dp} to various data collection and analysis scenarios\cite{kasiviswanathan2010learn}.

\section{Fluent Bit}
Fluent Bit is an industry-standard open source Log and Metric processor capable of taking in events from a large variety of sources, applying filters, buffering, and then sending the processed data to a variety of destinations.\cite{What-is-Fluent-Bit} Written in C, it's YAML-based configuration defines a graph structure for where events should be sourced, what to do with the event, and where the event should be eventually sent. For example: a hub device running Fluent Bit could take in logs from a connected IoT device via MQTT, it then can extract a metric contained in the string, create a histogram based on those stored values, buffer the values to a persistent disk, then send the stored data to a remote server over your desired protocol.
\subsection{Other WASM plugins}
An example of a plugin that leverages Fluent Bit's WASM filter plugin support is \textit{flb\_filter\_iis}\cite{Ortega} which processes W3C's IIS-formatted logs into JSON to be consumed by Fluent Bit.

\section{Existing Implementations of Differential Privacy}

Differential privacy is a mathematical framework for protecting the privacy of individuals in a dataset while still allowing statistical analysis and insights to be derived from the data \cite{Dwork2006calibrating, Dwork2014}. There are a number of existing software implementations that aim to make differential privacy more practical and accessible to data analysts and scientists. This section reviews some of the most notable implementations.

\subsection{Google's Differential Privacy Library}

The genesis of Google's Differential Privacy Library can be traced back to its earlier privacy project, RAPPOR, which aimed to improve privacy technologies for Chrome in 2014. Google's commitment to differential privacy has evolved significantly since then, leading to the launch of its open-source Differential Privacy library in 2019. This library is written in C++ and allows developers to implement differential privacy in their applications, enabling them to gain useful insights from data without compromising individual privacy \cite{Guevara_2022, RAPPOR, wilson2019differentially}.

Google's open-source library provides a comprehensive suite of tools and features to facilitate the implementation of differential privacy. These include a number of \acrshort{dp} mechanisms and algorithms for generating \acrshort{dp} releases, such as the Laplace mechanism, exponential mechanism, and noisy max. It also includes methods for computing \acrshort{dp} bounds, performing composition to track the overall privacy budget across multiple releases, statistical functions like counts, sums, averages, medians, and percentiles, rigorous testing frameworks to ensure the accuracy of \acrshort{dp} implementations, and modular components for extended functionalities \cite{wilson2019differentially, wilson2020differentially, RAPPOR}. The library supports a variety of programming languages, including C++, Java, Go, and Python, significantly broadening its accessibility and potential impact.

The open-source library has been utilized in various Google products to introduce privacy-preserving features. For example, it powers functions in Google Maps that inform users about the busyness of a place or the popularity of a dish without revealing any personal data. Google has also integrated this library into its BigQuery database platform to perform differentially private database queries on large datasets \cite{Google_BigQuery}.

\subsection{Apple's Differential Privacy}

Apple has been a pioneer in deploying differential privacy at scale in its products and services. The company first announced its use of \acrshort{dp} in 2016 for collecting user data to improve services like QuickType, emoji suggestions, and Spotlight search \cite{tang2017privacy, AppleDifferentialPrivacy2017}. Apple's \acrshort{dp} implementation relies on a local model, where noise is added to data on the user's device before being sent to Apple's servers. This ensures that Apple never sees the raw data and can only learn aggregate statistics.

Apple's \acrshort{dp} technology is built into the core of its operating systems, including iOS, macOS, and tvOS. It is used for a variety of purposes, such as discovering new words for QuickType, identifying problematic websites in Safari, and measuring the effectiveness of search results. Apple has published technical whitepapers detailing its \acrshort{dp} implementation and privacy guarantees \cite{AppleDifferentialPrivacy2017}.

\subsection{Microsoft's Differential Privacy Implementation}

Microsoft has been a significant contributor to the field of differential privacy through both theoretical research and practical implementations. A notable contribution from Microsoft is the development of the SmartNoise system as part of their Differential Privacy Toolkit, which was developed in collaboration with Harvard's Institute for Quantitative Social Science (IQSS) and School of Engineering and Applied Sciences (SEAS) as part of the OpenDP initiative \cite{MicrosoftOpenDP, MicrosoftHarvardOpenDP}. This platform is designed to enable researchers and data scientists to derive insights from datasets while preserving individual privacy. SmartNoise achieves this by adding statistical noise to the data and managing a privacy-loss budget to ensure that individual contributions are masked without significantly affecting the overall accuracy of the data analysis.

Microsoft's differential privacy framework comprises two main components: statistical noise and a privacy-loss budget. Statistical noise helps mask the contributions of individual data points within a dataset, maintaining the dataset's overall accuracy. The privacy-loss budget tracks the amount of information revealed through various queries, preventing the inadvertent disclosure of private information \cite{MicrosoftDataResponsibly}.

Notable applications of Microsoft's differential privacy technology include Windows telemetry data and LinkedIn advertiser queries. By applying \acrshort{dp}, Microsoft aims to understand app usage patterns and provide advertisers with insights while reducing the risk of compromising user privacy \cite{MicrosoftDataResponsibly, ding2017collecting}.

While Microsoft's \acrshort{dp} efforts have limited direct applicability to the Fluent Bit filter plugin, as they focus on getting differentially private insights from a private dataset with a trusted curator, their work demonstrates the feasibility of deploying \acrshort{dp} in production environments. Microsoft has also contributed to the development of the OpenDP framework, pushing the envelope of \acrshort{dp} research and implementation \cite{MicrosoftSmartNoise}.

\subsection{US Census Bureau's Differential Privacy}

The US Census Bureau has adopted differential privacy as the new standard for protecting the privacy of individuals in its data releases. Starting with the 2020 Census, the Bureau is using \acrshort{dp} to add noise to the data to prevent the identification of individuals while still allowing accurate population statistics to be computed \cite{Gong2022Harnessing}.

The Census Bureau's \acrshort{dp} implementation is based on the TopDown algorithm, which adds noise to the data in a hierarchical manner, starting with the national level and working down to the block level. The algorithm is designed to ensure that the noise added at each level is consistent with the noise added at higher levels, preserving the accuracy of the data as much as possible \cite{Gong2022Harnessing}.

The adoption of \acrshort{dp} by the Census Bureau represents a significant milestone for the technology, as it demonstrates its feasibility for use in large-scale, high-stakes applications. The Bureau's implementation has undergone extensive testing and validation to ensure that it meets the Bureau's strict accuracy and privacy requirements.

\subsection{OpenDP}

OpenDP is a community effort to build an open-source suite of tools for deploying differential privacy \cite{opendp2020, Vadhan2019OpenDPA}. It consists of several projects and libraries in multiple languages, including a Rust-based library developed by researchers from Harvard that implements many common \acrshort{dp} algorithms.

OpenDP aims to address several shortcomings in existing \acrshort{dp} implementations, such as being tailored to specific applications, requiring extensive expertise in computer science or \acrshort{dp}, and lacking vetting by the broader \acrshort{dp} community. The goal is to create a set of general-purpose, usable, and scalable tools for \acrshort{dp} that users can have confidence in.

The OpenDP community envisions the project as a hub for collaboration and contributions from academia and industry. By incorporating code and ideas from various existing \acrshort{dp} projects, OpenDP aims to build a robust, trustworthy, and widely-adopted toolkit for \acrshort{dp}. The project has already received contributions from several academic and industry partners, including Microsoft's SmartNoise system.

\subsection{IBM's Diffprivlib}

IBM has created Diffprivlib, an open-source library in Python for experimenting with differential privacy \cite{holohan2019diffprivlib}. It implements several \acrshort{dp} mechanisms, including the Laplace and Gaussian mechanisms, along with some classical \acrshort{dp} algorithms like noisy histograms and k-means clustering. The library is designed for ease-of-use and has extensive documentation and examples.

Diffprivlib is intended to be an accessible tool for researchers, data scientists, and developers who want to learn about and experiment with \acrshort{dp}. It provides a simple and intuitive API for applying \acrshort{dp} to data analysis tasks, making it a good choice for those new to the field.


\subsection{Problems in Differential Privacy}
\begin{enumerate}
    \item 
\end{enumerate}

\section{Algorithms}

